```
cd mediawiki2book &&  npm i 
```

```
cd lib && npm start
```

### 一点儿坑

#### 关于爬虫

1. 爬虫本身的逻辑是从Kibana的页面中爬取ES的数据，爬的接口的条件中，只有title字段是可以被传递，时间字段没有修改，在代码的TODO中已标注

2. 目前爬虫的频率是1000ms之后，从主站拉去，防止压力过大

3. 目前爬取数据的时候，ES提供的Filter无法精确匹配（文档没查到相关说明），提供的数据比较多，解决方案思路比较清晰，换成拼接的地址`main.xxx/xxxx`无多级路径，wiki都是自命名，相对比较容易，可以换成Puppeteer来抓取页面内容，还要啥Kibana爬虫。。

#### 关于 file.sh

1. 脚本的主要能力是用来解压 images 文件，给转换pdf提供图片资源（准确的说是所有资源），解决的主要问题是把多层级的目录扁平化，使用方式：
```
    sodu chmod -R 777 file.sh //授权
    ./file.sh $src $dest //都是绝对地址
```

